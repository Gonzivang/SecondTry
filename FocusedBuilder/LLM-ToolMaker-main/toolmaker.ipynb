{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import openai\n",
    "import itertools\n",
    "import random\n",
    "openai.api_key = input(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_tokens=256, temperature=0.0, model=\"gpt-3.5-turbo\"):\n",
    "    if model in [\"gpt-3.5-turbo\", \"gpt-4\"]:\n",
    "        params = {\n",
    "            \"model\": model,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "        }\n",
    "        for retry in range(3):\n",
    "            try:\n",
    "                return openai.ChatCompletion.create(**params)[\"choices\"][0][\"message\"][\"content\"]\n",
    "            except:\n",
    "                pass\n",
    "        raise Exception(\"Failed to generate\")\n",
    "    \n",
    "    # For older models, use the completion API with max_tokens=1024\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": min(max_tokens, 1024),\n",
    "        \"temperature\": temperature,\n",
    "        \"prompt\": prompt\n",
    "    }\n",
    "    for retry in range(3):\n",
    "        try:\n",
    "            return openai.Completion.create(**params)[\"choices\"][0][\"text\"]\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task(task):\n",
    "    with open(f\"bbh/{task}.json\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # For dyck languages task, we need remove the spaces in the inputs to avoid unnecessary issues with tokenization\n",
    "    if task == \"dyck_languages\":\n",
    "        for example in data[\"examples\"]:\n",
    "            desc, input = example[\"input\"].split(\"Input: \")\n",
    "            input = input.replace(\" \", \"\")\n",
    "            example[\"input\"] = f\"{desc}Input: {input}\"\n",
    "            example[\"target\"] = example[\"target\"].replace(\" \", \"\")\n",
    "    \n",
    "    train = []\n",
    "    val = []\n",
    "    test = []\n",
    "    for index in range(len(data['examples'])):\n",
    "        sample = {\n",
    "            'question': data['examples'][index]['input'],\n",
    "            'answer': data['examples'][index]['target'],\n",
    "        }\n",
    "        if index < 5:\n",
    "            train.append(sample)\n",
    "        elif index < 10:\n",
    "            val.append(sample)\n",
    "        else:\n",
    "            test.append(sample)\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_maker_prompt = \"\"\"Please write a generic Python function to solve this type of problems using only standard python libraries. The output of the function can later be converted to the answer (option for multiple choice question). All the function should be wrapped by\n",
    "```python\n",
    "```\"\"\"\n",
    "\n",
    "tool_verification_prompt = \"\"\"Write unit tests to verify the correctness of the function on the questions above using the following format:\n",
    "```python\n",
    "{parse the question into the arguments of the function}\n",
    "{call the function and save the return value in a variable named `ret`}\n",
    "{for multiple choice question, parse the options}\n",
    "{convert the return value `ret` to the answer (if the question is a multiple choice question, convert to an option) and save it in a variable named `ans`, otherwise}\n",
    "{assert ans == the provided answer (if the question is a multiple choice question, assert ans == option)}\n",
    "```\"\"\"\n",
    "\n",
    "tool_wrapper_prompt = \"\"\"Success! The function is correct. We will need to summarize the function and use cases up for further use. Please extract the information from the history in the following format:\n",
    "\n",
    "Here is a function to solve a class of problems:\n",
    "```python\n",
    "{the function, including necessary imports}\n",
    "```\n",
    "\n",
    "Use cases:\n",
    "Question: {question (including options)}\n",
    "Solution:\n",
    "```python\n",
    "{parse the question into the arguments of the function}\n",
    "{call the function and save the return value in a variable named `ret`}\n",
    "{for multiple choice question, parse the options}\n",
    "{convert the return value `ret` to the answer (if the question is a multiple choice question, convert to an option) and save it in a variable named `ans`, otherwise}\n",
    "```\n",
    "Do this for all the questions in the verification step.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def tool_making(train, val, train_samples=3, val_samples=3, model=\"gpt-4\", temperature=0.3):\n",
    "    prompt1 = \"\\n\\n\".join([f\"Question: {sample['question']}\\nAnswer: {sample['answer']}\" for sample in train[:train_samples]]) + \"\\n\\n\" + tool_maker_prompt\n",
    "    message = [{\"role\": \"user\", \"content\": prompt1}]\n",
    "\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 2048,\n",
    "        \"temperature\": temperature,\n",
    "        \"messages\": message\n",
    "    }\n",
    "\n",
    "    for retry in range(3):\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(**params)[\"choices\"][0][\"message\"][\"content\"]\n",
    "            message.append({\"role\": \"assistant\", \"content\": response})\n",
    "            tool = \"\\n\\n\".join(re.findall(r\"```python\\n(.*?)```\", response, re.DOTALL))\n",
    "            exec(tool, globals())\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"ERROR: failed to generate tool\", e)\n",
    "            message.append({\"role\": \"user\", \"content\": f\"Failed to execute the function due to the error: {type(e).__name__} {e}. Please fix it and try again.\"})\n",
    "\n",
    "    print(\"Tool:\", message[-1][\"content\"])\n",
    "        \n",
    "    message.append({\"role\": \"assistant\", \"content\": response})\n",
    "    \n",
    "    prompt2 = \"\\n\\n\".join([f\"Question: {sample['question']}\\nAnswer: {sample['answer']}\" for sample in val[:val_samples]]) + \"\\n\\n\" + tool_verification_prompt\n",
    "\n",
    "    message.append({\"role\": \"user\", \"content\": prompt2})\n",
    "\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 2048,\n",
    "        \"temperature\": temperature,\n",
    "        \"messages\": message\n",
    "    }\n",
    "\n",
    "    success = False\n",
    "\n",
    "    for retry in range(3):\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(**params)[\"choices\"][0][\"message\"][\"content\"]\n",
    "            message.append({\"role\": \"assistant\", \"content\": response})\n",
    "            verification = \"\\n\\n\".join(re.findall(r\"```python\\n(.*?)```\", response, re.DOTALL))\n",
    "            exec(tool+\"\\n\"+verification, globals())\n",
    "            success = True\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"ERROR: failed to verify\", e)\n",
    "            message.append({\"role\": \"user\", \"content\": f\"Failed to verify the function due to the error: {type(e).__name__} {e}. Please fix it and try again.\"})\n",
    "\n",
    "    print(\"Verification:\", message[-1][\"content\"])\n",
    "\n",
    "    if success:\n",
    "        message.append({\"role\": \"user\", \"content\": tool_wrapper_prompt})\n",
    "        params = {\n",
    "            \"model\": model,\n",
    "            \"max_tokens\": 2048,\n",
    "            \"temperature\": temperature,\n",
    "            \"messages\": message\n",
    "        }\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(**params)[\"choices\"][0][\"message\"][\"content\"]\n",
    "            message.append({\"role\": \"assistant\", \"content\": response})\n",
    "            print(\"Wrapper:\", response)\n",
    "        except Exception as e:\n",
    "            print(\"ERROR: failed to generate wrapper\", e)\n",
    "    return tool, verification, success, message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"schedule_meeting\"\n",
    "#\"tracking_shuffled_objects_five_objects\"\n",
    "#\"tracking_shuffled_objects_seven_objects\"\n",
    "#\"logical_deduction_five_objects\"\n",
    "#\"logical_deduction_seven_objects\"\n",
    "#\"dyck_languages\"\n",
    "#\"word_sorting\"\n",
    "#\"chinese_remainder_theorem\"\n",
    "#\"schedule_meeting\"\n",
    "model = \"gpt-4\"\n",
    "train, val, test = get_task(task)\n",
    "\n",
    "tool, verification, success, message = tool_making(train, val, train_samples=3, val_samples=3, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(message, f\"tools/{task}.pt\")\n",
    "# Dump to json\n",
    "json.dump(message, open(f\"tools/{task}.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
